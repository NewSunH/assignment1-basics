\documentclass[12pt,UTF8,fontset=none,zihao=5]{ctexart}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{seqsplit}

\usepackage{fontspec}

% Allow line breaks in very long \texttt{...} strings (e.g., longest tokens).
\newcommand{\ttbreak}[1]{\texttt{\seqsplit{#1}}}


% 中文字体（Noto Sans CJK）
\setCJKmainfont{Noto Serif CJK SC}
\setCJKsansfont{Noto Sans CJK SC}
\setCJKmonofont{Noto Sans Mono CJK SC}

\newtcolorbox{problem}[3][]{
  enhanced,
  colback=white,
  colframe=orange,
  coltitle=black,
  fonttitle=\bfseries,
  title=Problem (\texttt{#2}): #3,
  sharp corners,
  boxrule=1pt,
  left=6pt,
  right=6pt,
  top=6pt,
  bottom=6pt
}

\ctexset{
  section = {
    format = \Large\bfseries\raggedright,
    beforeskip = 1.5ex,
    afterskip = 1ex
  }
}


\setlist{nosep,topsep=4pt,itemsep=2pt,leftmargin=18pt}

\title{
      CS336 Assignment 1 (basics): Building a Transformer LM\\
      \Large Answer Sheet
}
\author{NewSunH}
\date{}

\begin{document}

\maketitle

\section{Assignment Overview}
 (nothing to solve)
\section{Byte-Pair Encoding (BPE) Tokenizer}

\subsection{The Unicode Standard}

\begin{problem}{unicode1}{Understanding Unicode (1 point)}
\begin{enumerate}[label=(\alph*)]
      \item What Unicode character does \texttt{chr(0)} return?\\
            \textbf{answer: }\texttt{'\textbackslash x00'}, or Unicode character \texttt{U+0000}, represents NUL (the null character).
      \item How does this character's string representation ( \texttt{ \_\_repr\_\_() } ) differ?\\
            \textbf{answer: }\texttt{'\textbackslash x00'} in string representation, and invisible when using \texttt{print()}.
      \item What happens when this character occurs in text?\\
            \textbf{answer: }Adding the NUL character to a string doesn't change what it looks like (in the most satuation). But there is actually a invisible character adding to the storage. When using UTF-8 typically, a zero byte (\texttt{0x00}) was added in the place of it.
\end{enumerate}
\end{problem}

\subsection{Unicode Encodings}

\begin{problem}{unicode2}{Unicode Encodings (3 points)}
\begin{enumerate}[label=(\alph*)]
      \item What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\\
            \textbf{answer: }UTF-8 is byte-based, and ASCII compatible. UTF-8 is much more effient than UTF-16/32, because UTF-16/32 using zeros to fill in the blanks, which will also influence tokens.
      \item Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\\
            \textbf{answer: }It literally decode each byte. UTF-8 character might be multiple bytes, e.g. Chinese characters and emoji. e.g. \texttt{'你好，世界'}.
      \item Give a two byte sequence that does not decode to any Unicode character(s).\\
            \textbf{answer: }e.g. \texttt{0x8080}, which is invalid because continuation bytes cannot appear without a valid leading byte.
\end{enumerate}
\end{problem}

\clearpage

\subsection{Subword Tokenization}
\subsection{BPE Tokenizer Training}
(nothing to solve)

\subsection{Experimenting with BPE Tokenizer Training}

\begin{problem}{train\_bpe}{BPE Tokenizer Training (15 points)}
implemented in \texttt{\slash cs336\_basics\slash bpe.py}
\end{problem}

\begin{problem}{train\_bpe\_tinystories}{BPE Training on TinyStories (2 points)}
\begin{enumerate}[label=(\alph*)]
      \item Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories \texttt{<|endoftext|>} special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\\
            \textbf{answer: }It takes 117.98s, 10GB RAM in total. The longest token is \texttt{Ġaccomplishment}, which is 15 bytes long and its id is \texttt{7160}.
      \item Profle your code. What part of the tokenizer training process takes the most time?\\
            \textbf{answer: }In this dataset, pretokenize takes 1:19s, over 2/3 of total time was taken.

\end{enumerate}
\end{problem}

\begin{problem}{train\_bpe\_expts\_owt}{BPE Training on OpenWebText (2 points)}
\begin{enumerate}[label=(\alph*)]
      \item Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What is the longest token in the vocabulary? Does it make sense?\\
            \textbf{answer: }It takes 4810.98s (1h 20m 10.98s). The longest token is \ttbreak{ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ} (strange, I can't tell what it is) and \ttbreak{----------------------------------------------------------------} (64 dashes). It does make sence because I found them in the owt.
      \item Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\\
            \texttt{bytes/token (lower is better compression)\\
                  TS tok on TS: 3.979507\\
                  TS tok on OWT: 3.159600\\
                  OWT tok on TS: 3.890763\\
                  OWT tok on OWT: 4.344127}
\end{enumerate}
\end{problem}

\subsection{BPE Tokenizer: Encoding and Decoding}
\begin{problem}{tokenizer}{Implementing the tokenizer (15 points)}
implemented in \texttt{/cs336\_basics/tokenizer.py}
\end{problem}

\subsection{Experiments}

\begin{problem}{tokenizer\_experiments}{Experiments with tokenizers (4 points)}
\begin{enumerate}[label=(\alph*)]
      \item Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\\
            \textbf{answer: }I sampled 10 documents (lines) from each \texttt{*-valid.txt} file and measured bytes/token (lower is better). TinyStories tokenizer on TinyStories: \texttt{3.762821} bytes/token; OpenWebText tokenizer on OpenWebText: \texttt{4.308458} bytes/token.
      \item What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\\
            \textbf{answer: }On the same 10-doc OpenWebText sample, using the TinyStories tokenizer gives \texttt{2.955631} bytes/token, which is \emph{lower} than using the OpenWebText tokenizer (\texttt{4.308458}). Qualitatively, the OWT tokenizer learns some very long/rare byte tokens from noisy web text, while the TinyStories tokenizer tends to break these into smaller pieces, which in this sample leads to more effective compression.
      \item Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\\
            \textbf{answer: }Measured on 50MB of \texttt{*-valid.txt}, throughput is about \texttt{5.76e6} bytes/s (TinyStories tokenizer on TinyStories valid) and \texttt{3.70e6} bytes/s (OWT tokenizer on OWT valid). At \texttt{3.70e6} bytes/s, tokenizing 825GB takes about \texttt{66.6} hours.
      \item Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\\
            \textbf{answer: }\texttt{uint16} can represent IDs up to 65535, which comfortably covers vocab sizes 10K/32K, and it halves storage compared to \texttt{int32} (2 bytes vs 4 bytes per token). This matters because the token ID sequences are very large and will be memory-mapped for training.

\end{enumerate}
\end{problem}

\section{Transformer Language Model Architecture}
\subsection{Transformer LM}
\subsection{Output Normalization and Embedding}
\subsection{Remark: Batching, Einsum and Efficient Computation}
(nothing to solve)

\subsection{Basic Building Blocks: Linear and Embedding Modules}
\begin{problem}{linear}{Implementing the linear module (1 point)}
implemented in \texttt{/cs336\_basics/linear.py}
\end{problem}

\begin{problem}{embedding}{Implement the embedding module (1 point)}
implemented in \texttt{/cs336\_basics/embedding.py}
\end{problem}

\subsection{Pre-Norm Transformer Block}
\begin{problem}{rmsnorm}{Root Mean Square Layer Normalization (1 point)}
implemented in \texttt{/cs336\_basics/normalization.py}
\end{problem}

\begin{problem}{positionwise\_feedforward}{Implement the position-wise feed-forward network (2 points)}
implemented in \texttt{/cs336\_basics/positionwise\_feedforward.py}
\end{problem}

\begin{problem}{rope}{Implement RoPE (2 points)}
implemented in \texttt{/cs336\_basics/rope.py}
\end{problem}

\begin{problem}{softmax}{Implement softmax (1 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{scaled\_dot\_product\_attention}{Implement scaled dot-product attention (5 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{multihead\_self\_attention}{Implement causal multi-head self-attention}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\subsection{The Full Transformer LM}

\begin{problem}{transformer\_block}{Implement the Transformer block (3 points)}
implemented in \texttt{/cs336\_basics/transformer.py}
\end{problem}

\begin{problem}{transformer\_lm}{Implement the Transformer block (3 points)}
implemented in \texttt{/cs336\_basics/transformer.py}
\end{problem}


\begin{problem}{learning\_rate\_tuning}{Tuning the learning rate (1 point)}
As we will see, one of the hyperparameters that affects training the most is the learning rate. Let’s see that in practice in our toy example. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?\\
\textbf{answer: }Running the toy SGD loop for 10 iterations, \texttt{lr=1} decreases the loss slowly while \texttt{lr=10} decreases it much faster. \texttt{lr=100} drives the loss to (near) zero within a few steps, but \texttt{lr=1000} diverges immediately and the loss explodes.
\end{problem}

\end{document}
