\documentclass[12pt,UTF8,fontset=none,zihao=5]{ctexart}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{fontspec}


% 中文字体（Noto Sans CJK）
\setCJKmainfont{Noto Serif CJK SC}
\setCJKsansfont{Noto Sans CJK SC}
\setCJKmonofont{Noto Sans Mono CJK SC}

\newtcolorbox{problem}[3][]{
  enhanced,
  colback=white,
  colframe=orange,
  coltitle=black,
  fonttitle=\bfseries,
  title=Problem (\texttt{#2}): #3,
  sharp corners,
  boxrule=1pt,
  left=6pt,
  right=6pt,
  top=6pt,
  bottom=6pt
}

\ctexset{
  section = {
    format = \Large\bfseries\raggedright,
    beforeskip = 1.5ex,
    afterskip = 1ex
  }
}


\setlist{nosep,topsep=4pt,itemsep=2pt,leftmargin=18pt}

\title{
      CS336 Assignment 1 (basics): Building a Transformer LM\\
      \Large Answer Sheet
}
\author{NewSunH}
\date{}

\begin{document}

\maketitle

\section{Assignment Overview}
 (nothing to solve)
\section{Byte-Pair Encoding (BPE) Tokenizer}

\subsection{The Unicode Standard}

\begin{problem}{unicode1}{Understanding Unicode (1 point)}
\begin{enumerate}[label=(\alph*)]
      \item What Unicode character does \texttt{chr(0)} return?\\
            \textbf{answer: }\texttt{'\textbackslash x00'}, or Unicode character \texttt{U+0000}, represents NUL (the null character).
      \item How does this character's string representation ( \texttt{ \_\_repr\_\_() } ) differ?\\
            \textbf{answer: }\texttt{'\textbackslash x00'} in string representation, and invisible when using \texttt{print()}.
      \item What happens when this character occurs in text?\\
            \textbf{answer: }Adding the NUL character to a string doesn't change what it looks like (in the most satuation). But there is actually a invisible character adding to the storage. When using UTF-8 typically, a zero byte (\texttt{0x00}) was added in the place of it.
\end{enumerate}
\end{problem}

\subsection{Unicode Encodings}

\begin{problem}{unicode2}{Unicode Encodings (3 points)}
\begin{enumerate}[label=(\alph*)]
      \item What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\\
            \textbf{answer: }UTF-8 is byte-based, and ASCII compatible. UTF-8 is much more effient than UTF-16/32, because UTF-16/32 using zeros to fill in the blanks, which will also influence tokens.
      \item Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\\
            \textbf{answer: }It literally decode each byte. UTF-8 character might be multiple bytes, e.g. Chinese characters and emoji. e.g. \texttt{'你好，世界'}.
      \item Give a two byte sequence that does not decode to any Unicode character(s).\\
            \textbf{answer: }e.g. \texttt{0x8080}, which is invalid because continuation bytes cannot appear without a valid leading byte.
\end{enumerate}
\end{problem}

\clearpage

\subsection{Subword Tokenization}
\subsection{BPE Tokenizer Training}
(nothing to solve)

\subsection{Experimenting with BPE Tokenizer Training}

\begin{problem}{train\_bpe}{BPE Tokenizer Training (15 points)}
implemented in \texttt{\slash cs336\_basics\slash bpe.py}
\end{problem}

\begin{problem}{train\_bpe\_tinystories}{BPE Training on TinyStories (2 points)}
\begin{enumerate}[label=(\alph*)]
      \item Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories \texttt{<|endoftext|>} special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\\
            \textbf{answer: }It takes 117.98s, 10GB RAM in total. The longest token is \texttt{Ġaccomplishment}, which is 15 bytes long and its id is \texttt{7160}.
      \item Profle your code. What part of the tokenizer training process takes the most time?\\
            \textbf{answer: }In this dataset, pretokenize takes 1:19s, over 2/3 of total time was taken.

\end{enumerate}
\end{problem}

\begin{problem}{train\_bpe\_expts\_owt}{BPE Training on OpenWebText (2 points)}
(untrained, that's too demanding.)
\end{problem}

\subsection{BPE Tokenizer: Encoding and Decoding}
\begin{problem}{tokenizer}{Implementing the tokenizer (15 points)}
implemented in \texttt{/cs336\_basics/tokenizer.py}
\end{problem}

\subsection{Experiments}

\begin{problem}{tokenizer\_experiments}{Experiments with tokenizers (4 points)}
\begin{enumerate}[label=(\alph*)]
      \item Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\\
            unfinished
      \item

\end{enumerate}
\end{problem}

\section{Transformer Language Model Architecture}
\subsection{Transformer LM}
\subsection{Output Normalization and Embedding}
\subsection{Remark: Batching, Einsum and Efficient Computation}
(nothing to solve)

\subsection{Basic Building Blocks: Linear and Embedding Modules}
\begin{problem}{linear}{Implementing the linear module (1 point)}
implemented in \texttt{/cs336\_basics/linear.py}
\end{problem}

\begin{problem}{embedding}{Implement the embedding module (1 point)}
implemented in \texttt{/cs336\_basics/embedding.py}
\end{problem}

\subsection{Pre-Norm Transformer Block}
\begin{problem}{rmsnorm}{Root Mean Square Layer Normalization (1 point)}
implemented in \texttt{/cs336\_basics/normalization.py}
\end{problem}

\begin{problem}{positionwise\_feedforward}{Implement the position-wise feed-forward network (2 points)}
implemented in \texttt{/cs336\_basics/positionwise\_feedforward.py}
\end{problem}

\begin{problem}{rope}{Implement RoPE (2 points)}
implemented in \texttt{/cs336\_basics/rope.py}
\end{problem}

\begin{problem}{softmax}{Implement softmax (1 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{scaled\_dot\_product\_attention}{Implement scaled dot-product attention (5 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{multihead\_self\_attention}{Implement causal multi-head self-attention}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\end{document}
