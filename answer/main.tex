\documentclass[12pt,UTF8,fontset=none,zihao=5]{ctexart}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{unicode-math}
\setmathfont{Latin Modern Math}
\usepackage{seqsplit}

\usepackage{fontspec}

% Allow line breaks in very long \texttt{...} strings (e.g., longest tokens).
\newcommand{\ttbreak}[1]{\texttt{\seqsplit{#1}}}


% 中文字体（Noto Sans CJK）
\setCJKmainfont{Noto Serif CJK SC}
\setCJKsansfont{Noto Sans CJK SC}
\setCJKmonofont{Noto Sans Mono CJK SC}

\newtcolorbox{problem}[3][]{
      enhanced,
      breakable,
  colback=white,
  colframe=orange,
  coltitle=black,
  fonttitle=\bfseries,
  title=Problem (\texttt{#2}): #3,
  sharp corners,
  boxrule=1pt,
  left=6pt,
  right=6pt,
  top=6pt,
  bottom=6pt
}

\ctexset{
  section = {
    format = \Large\bfseries\raggedright,
    beforeskip = 1.5ex,
    afterskip = 1ex
  }
}


\setlist{nosep,topsep=4pt,itemsep=2pt,leftmargin=18pt}

\title{
      CS336 Assignment 1 (basics): Building a Transformer LM\\
      \Large Answer Sheet
}
\author{NewSunH}
\date{}

\begin{document}

\maketitle

\section{Assignment Overview}
 (nothing to solve)
\section{Byte-Pair Encoding (BPE) Tokenizer}

\subsection{The Unicode Standard}

\begin{problem}{unicode1}{Understanding Unicode (1 point)}
\begin{enumerate}[label=(\alph*)]
      \item What Unicode character does \texttt{chr(0)} return?\\
            \textbf{answer: }\texttt{'\textbackslash x00'}, or Unicode character \texttt{U+0000}, represents NUL (the null character).
      \item How does this character's string representation ( \texttt{ \_\_repr\_\_() } ) differ?\\
            \textbf{answer: }\texttt{'\textbackslash x00'} in string representation, and invisible when using \texttt{print()}.
      \item What happens when this character occurs in text?\\
            \textbf{answer: }Adding the NUL character to a string doesn't change what it looks like (in the most satuation). But there is actually a invisible character adding to the storage. When using UTF-8 typically, a zero byte (\texttt{0x00}) was added in the place of it.
\end{enumerate}
\end{problem}

\subsection{Unicode Encodings}

\begin{problem}{unicode2}{Unicode Encodings (3 points)}
\begin{enumerate}[label=(\alph*)]
      \item What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\\
            \textbf{answer: }UTF-8 is byte-based, and ASCII compatible. UTF-8 is much more effient than UTF-16/32, because UTF-16/32 using zeros to fill in the blanks, which will also influence tokens.
      \item Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\\
            \textbf{answer: }It literally decode each byte. UTF-8 character might be multiple bytes, e.g. Chinese characters and emoji. e.g. \texttt{'你好，世界'}.
      \item Give a two byte sequence that does not decode to any Unicode character(s).\\
            \textbf{answer: }e.g. \texttt{0x8080}, which is invalid because continuation bytes cannot appear without a valid leading byte.
\end{enumerate}
\end{problem}

\clearpage

\subsection{Subword Tokenization}
\subsection{BPE Tokenizer Training}
(nothing to solve)

\subsection{Experimenting with BPE Tokenizer Training}

\begin{problem}{train\_bpe}{BPE Tokenizer Training (15 points)}
implemented in \texttt{\slash cs336\_basics\slash bpe.py}
\end{problem}

\begin{problem}{train\_bpe\_tinystories}{BPE Training on TinyStories (2 points)}
\begin{enumerate}[label=(\alph*)]
      \item Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories \texttt{<|endoftext|>} special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\\
            \textbf{answer: }It takes 117.98s, 10GB RAM in total. The longest token is \texttt{Ġaccomplishment}, which is 15 bytes long and its id is \texttt{7160}.
      \item Profle your code. What part of the tokenizer training process takes the most time?\\
            \textbf{answer: }In this dataset, pretokenize takes 1:19s, over 2/3 of total time was taken.

\end{enumerate}
\end{problem}

\begin{problem}{train\_bpe\_expts\_owt}{BPE Training on OpenWebText (2 points)}
\begin{enumerate}[label=(\alph*)]
      \item Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What is the longest token in the vocabulary? Does it make sense?\\
            \textbf{answer: }It takes 4810.98s (1h 20m 10.98s). The longest token is \ttbreak{ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ} (strange, I can't tell what it is) and \ttbreak{----------------------------------------------------------------} (64 dashes). It does make sence because I found them in the owt.
      \item Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\\
            \texttt{bytes/token (lower is better compression)\\
                  TS tok on TS: 3.979507\\
                  TS tok on OWT: 3.159600\\
                  OWT tok on TS: 3.890763\\
                  OWT tok on OWT: 4.344127}
\end{enumerate}
\end{problem}

\subsection{BPE Tokenizer: Encoding and Decoding}
\begin{problem}{tokenizer}{Implementing the tokenizer (15 points)}
implemented in \texttt{/cs336\_basics/tokenizer.py}
\end{problem}

\subsection{Experiments}

\begin{problem}{tokenizer\_experiments}{Experiments with tokenizers (4 points)}
\begin{enumerate}[label=(\alph*)]
      \item Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\\
            \textbf{answer: }I sampled 10 documents (lines) from each \texttt{*-valid.txt} file and measured bytes/token (lower is better). TinyStories tokenizer on TinyStories: \texttt{3.762821} bytes/token; OpenWebText tokenizer on OpenWebText: \texttt{4.308458} bytes/token.
      \item What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\\
            \textbf{answer: }On the same 10-doc OpenWebText sample, using the TinyStories tokenizer gives \texttt{2.955631} bytes/token, which is \emph{lower} than using the OpenWebText tokenizer (\texttt{4.308458}). Qualitatively, the OWT tokenizer learns some very long/rare byte tokens from noisy web text, while the TinyStories tokenizer tends to break these into smaller pieces, which in this sample leads to more effective compression.
      \item Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\\
            \textbf{answer: }Measured on 50MB of \texttt{*-valid.txt}, throughput is about \texttt{5.76e6} bytes/s (TinyStories tokenizer on TinyStories valid) and \texttt{3.70e6} bytes/s (OWT tokenizer on OWT valid). At \texttt{3.70e6} bytes/s, tokenizing 825GB takes about \texttt{66.6} hours.
      \item Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\\
            \textbf{answer: }\texttt{uint16} can represent IDs up to 65535, which comfortably covers vocab sizes 10K/32K, and it halves storage compared to \texttt{int32} (2 bytes vs 4 bytes per token). This matters because the token ID sequences are very large and will be memory-mapped for training.

\end{enumerate}
\end{problem}

\section{Transformer Language Model Architecture}
\subsection{Transformer LM}
\subsection{Output Normalization and Embedding}
\subsection{Remark: Batching, Einsum and Efficient Computation}
(nothing to solve)

\subsection{Basic Building Blocks: Linear and Embedding Modules}
\begin{problem}{linear}{Implementing the linear module (1 point)}
implemented in \texttt{/cs336\_basics/linear.py}
\end{problem}

\begin{problem}{embedding}{Implement the embedding module (1 point)}
implemented in \texttt{/cs336\_basics/embedding.py}
\end{problem}

\subsection{Pre-Norm Transformer Block}
\begin{problem}{rmsnorm}{Root Mean Square Layer Normalization (1 point)}
implemented in \texttt{/cs336\_basics/normalization.py}
\end{problem}

\begin{problem}{positionwise\_feedforward}{Implement the position-wise feed-forward network (2 points)}
implemented in \texttt{/cs336\_basics/positionwise\_feedforward.py}
\end{problem}

\begin{problem}{rope}{Implement RoPE (2 points)}
implemented in \texttt{/cs336\_basics/rope.py}
\end{problem}

\begin{problem}{softmax}{Implement softmax (1 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{scaled\_dot\_product\_attention}{Implement scaled dot-product attention (5 points)}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\begin{problem}{multihead\_self\_attention}{Implement causal multi-head self-attention}
implemented in \texttt{/cs336\_basics/attention.py}
\end{problem}

\subsection{The Full Transformer LM}

\begin{problem}{transformer\_block}{Implement the Transformer block (3 points)}
implemented in \texttt{/cs336\_basics/transformer.py}
\end{problem}

\begin{problem}{transformer\_lm}{Implementing the Transformer LM (3 points)}
implemented in \texttt{/cs336\_basics/transformer.py}
\end{problem}

\begin{problem}{transformer\_accounting}{Transformer LM resource accounting (5 points)}
unfinished
\end{problem}

\section{Training a Transformer LM}
\subsection{Cross-entropy loss}

\begin{problem}{cross\_entropy}{Implement Cross entropy}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\subsection{The SGD Optimizer}

\begin{problem}{learning\_rate\_tuning}{Tuning the learning rate (1 point)}
As we will see, one of the hyperparameters that affects training the most is the learning rate. Let’s see that in practice in our toy example. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?\\
\textbf{answer: }Running the toy SGD loop for 10 iterations, \texttt{lr=1} decreases the loss slowly while \texttt{lr=10} decreases it much faster. \texttt{lr=100} drives the loss to (near) zero within a few steps, but \texttt{lr=1000} diverges immediately and the loss explodes.
\end{problem}

\subsection{AdamW}

\begin{problem}{adamw}{Implement AdamW (2 points)}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\begin{problem}{adamwAccounting}{Resource accounting for training with AdamW (2 points)}
unfinished
\end{problem}

\subsection{Learning rate scheduling}

\begin{problem}{learning\_rate\_schedule}{Implement cosine learning rate schedule with warmup}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\subsection{Gradient clipping}

\begin{problem}{gradient\_clipping}{Implement gradient clipping (1 point)}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\section{Training loop}

\subsection{Data Loader}
\begin{problem}{data\_loading}{Implement data loading (2 points)}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\subsection{Checkpointing}
\begin{problem}{checkpointing}{Implement model checkpointing (1 point)}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}

\subsection{Training loop}
\begin{problem}{training\_together}{Put it together (4 points)}
implemented in \texttt{/cs336\_basics/train\_transformer.py}
\end{problem}


\section{Generating text}
\begin{problem}{decoding}{Decoding (3 points)}
implemented in \texttt{/cs336\_basics/transformer.py} as \texttt{TransformerLm.generate}. It generates completions autoregressively from a user-provided prompt (\texttt{input\_ids}) until hitting \texttt{eos\_token\_id} (e.g. \texttt{<|endoftext|>}) or reaching \texttt{max\_new\_tokens}. It supports temperature scaling (\texttt{temperature<=0} falls back to greedy decoding) and top-p (nucleus) sampling via \texttt{top\_p}.
\end{problem}

\section{Experiments}
\subsection{How to Run Experiments and Deliverables}

\begin{problem}{experiment\_log}{Experiment logging (3 points)}
implemented in \ttbreak{/cs336\_basics/experiment\_logging.py}. The logger writes per-step metrics with both \texttt{step} and \ttbreak{wallclock\_s} to \ttbreak{metrics.json}, and also stores run metadata (\ttbreak{run.json}) and free-form notes (\texttt{notes.log}). My running experiment log is maintained in \ttbreak{/experiments/experiment\_log.md}.
\end{problem}

\subsection{TinyStories}

\begin{problem}{learning\_rate}{Tune the learning rate (3 points)}
The learning rate is one of the most important hyperparameters to tune. Taking the base model you’ve trained, answer the following questions:
\begin{enumerate}[label=(\alph*)]
      \item Perform a hyperparameter sweep over the learning rates and report the final losses (or note divergence if the optimizer diverges).\\
            \textbf{answer: }I ran a learning-rate sweep over \texttt{lr\_max} (peak learning rate in the warmup+cosine schedule) while keeping all other hyperparameters fixed (same 17M-ish model and \texttt{total\_steps=10000}, processing \texttt{327,680,000} tokens when training completes). The final validation losses are:

            \begin{center}
                  \begin{tabular}{lcc}
                        \toprule
                        \texttt{lr\_max} & final validation loss & note                          \\
                        \midrule
                        1e-4             & 2.7623                & converged                     \\
                        2e-4             & 2.0991                & converged                     \\
                        3e-4             & 1.8096                & converged                     \\
                        5e-4             & 1.5811                & converged                     \\
                        8e-4             & 1.4690                & converged                     \\
                        1e-3             & 1.4368                & converged (meets $\le 1.45$)  \\
                        3e-3             & 1.3713                & best run                      \\
                        3e-2             & 4.3973                & does not learn well           \\
                        30               & ---                   & diverged at step \texttt{188} \\
                        \bottomrule
                  \end{tabular}
            \end{center}

            (Reproducibility note: these correspond to run directories under \texttt{outputs/runs/}, e.g. the best run is \ttbreak{outputs/runs/20260130T032122Z}, and a divergence run is \ttbreak{outputs/runs/20260130T064527Z}.)
      \item Folk wisdom is that the best learning rate is at the edge of stability. Investigate how the point at which learning rates diverge is related to your best learning rate.\\
            \textbf{answer: }In this setup, catastrophic divergence happens only at extremely large \texttt{lr\_max} (e.g., \texttt{lr\_max=30} diverges quickly at step \texttt{188}). However, well before catastrophic divergence there is an \emph{effective} stability boundary where optimization stops making meaningful progress: with \texttt{lr\_max=3e-2} the run does not diverge, but the training loss barely decreases (about \texttt{5.66 \,$\to$\, 4.41}) and the final validation loss is very poor (\texttt{4.3973}).

            The best learning rate I found is \texttt{lr\_max=3e-3} (final validation loss \texttt{1.3713}). This is about one order of magnitude below the point where training becomes effectively unstable (\texttt{3e-2}), and it is also far below the catastrophic divergence point (\texttt{3e1}). Empirically, performance improves as \texttt{lr\_max} increases from \texttt{1e-4} up to a few \texttt{e-3}, and then degrades sharply by \texttt{3e-2}. This supports the edge of stability heuristic if we interpret the edge as the largest learning rate that still yields stable \emph{and} productive learning dynamics, not merely not produce \texttt{NaN}.
\end{enumerate}
\end{problem}

\begin{problem}{batch\_size\_experiment}{Batch size variations (1 point)}
Vary your batch size all the way from 1 to the GPU memory limit. Try at least a few batch sizes in between, including typical sizes like 64 and 128.

\textbf{answer: }I ran a batch size sweep with the same TinyStories 17M-ish model (ctx=256, 4L/ 16H/ 512d/ 1344ff) and the same training code as in the learning-rate sweep. For each batch size, I adjusted \ttbreak{lr\_max} (peak LR in the warmup+cosine schedule) using linear scaling from the best batch size 128 setting, and clipped it if it entered clearly unstable regimes.

\textbf{How I ran it:} I used a SLURM array job (see \ttbreak{scripts/slurm/batch\_size\_experiment\_array.sbatch}). It sweeps \texttt{batch\_size $\in$ \{1,4,16,64,128,256,512\}} and uses a roughly fixed token budget per run so that \texttt{batch\_size=1} is feasible.

\textbf{Baseline point (already measured):} with \texttt{batch\_size=128} and tuned \texttt{lr\_max=3e-3}, the run reaches final validation loss \texttt{1.3713} (run \ttbreak{outputs/runs/20260130T032122Z}).

\textbf{Results summary (fill from runs):} after the sweep completes, I summarize the best final validation loss per batch size via \texttt{uv run python experiments/batch\_size\_results.py}, and include learning curves (train/valid loss vs step) for each batch size.

\begin{center}
      \begin{tabular}{lcc}
            \toprule
            batch size & tuned \texttt{lr\_max} & best final validation loss \\
            \midrule
            1          & 2.34375e-5             & 6.5495                     \\
            4          & 9.375e-5               & 3.3322                     \\
            16         & 3.75e-4                & 2.1118                     \\
            64         & 1.5e-3                 & 1.8727                     \\
            128        & 3e-3                   & 1.3713                     \\
            256        & 6e-3                   & 1.9497                     \\
            512        & ---                    & OOM (GPU memory limit)     \\
            \bottomrule
      \end{tabular}
\end{center}

\textbf{Discussion (expected/observed pattern):} smaller batch sizes tend to produce noisier gradients: training loss curves look jagged, but sometimes validation can improve slightly due to implicit regularization. As batch size increases, optimization becomes smoother and (up to a point) allows a larger learning rate (linear scaling), but very large batch sizes can hurt generalization or require retuning \texttt{lr\_max} and warmup to avoid getting stuck. In practice, I found that \texttt{batch\_size=128} is a good trade-off between throughput and validation loss for this model.

\end{problem}

\begin{problem}{generate}{Generate text (1 point)}
Using your decoder and your trained checkpoint, report the text generated by your model. You may need to manipulate decoder parameters (temperature, top-p, etc.) to get fluent outputs.

\textbf{answer: }I generated text using the best TinyStories checkpoint (run \ttbreak{outputs/runs/20260130T032122Z}, \ttbreak{checkpoints/final.pt}) and my implementation of \texttt{TransformerLm.generate}. In practice, this model assigns very high probability to the \texttt{<|endoftext|>} token immediately after short prompts, so to obtain a 256-token sample I disabled sampling of the EOS token during decoding (equivalent to forbidding EOS).

\textbf{Decoding settings:} prompt = \texttt{Once upon a time, }, max\_new\_tokens = \texttt{512} (output contains \(\ge 256\) tokens), temperature = \texttt{0.9}, top-p = \texttt{0.95}, and EOS is forbidden.

\textbf{Repro command:} \ttbreak{uv run python experiments/generate\_text.py --run-dir outputs/runs/20260130T032122Z --prompt "Once upon a time, " --max-new-tokens 512 --temperature 0.9 --top-p 0.95 --ban-eos}

\textbf{Generated text (\(\ge 256\) tokens):}
\inputminted[fontsize=\small,breaklines]{text}{../outputs/runs/20260130T032122Z/generated_text_clean.txt}

\textbf{Comment on fluency:} the output is partially coherent (it maintains a simple narrative about a journey), but it has clear issues: repetition of names ("Bill and Bill"), occasional punctuation/grammar glitches, and it does not develop a strong plot. Two important factors affecting quality are \emph{decoding hyperparameters} (higher temperature / larger top-p increases diversity but can harm coherence; lower values make it more repetitive), and \emph{stopping/handling of EOS} (this model strongly prefers emitting \texttt{<|endoftext|>} early; if EOS is allowed, generations often end immediately). Prompt specificity and checkpoint quality (validation loss) also strongly affect fluency.
\end{problem}

\subsection{Ablations and architecture modification}

\begin{problem}{layer\_norm\_ablation}{Remove RMSNorm and train (1 point)}
Remove all of the RMSNorms from your Transformer and train. What happens at the previous optimal learning rate? Can you get stability by using a lower learning rate?

\textbf{answer: }I implemented an ablation switch to remove all RMSNorm layers (replacing them with identity) and retrained on TinyStories.

\textbf{Runs:}
\begin{itemize}
      \item Baseline (with RMSNorm): \ttbreak{outputs/runs/20260130T032122Z} with \texttt{lr\_max=3e-3}.
      \item Ablation (no RMSNorm) at previous optimal LR: \ttbreak{outputs/runs/layer\_norm\_ablation\_norn\_lr3e-3\_steps2000\_seed0}.
      \item Ablation (no RMSNorm) best stable LR from a short sweep: \ttbreak{outputs/runs/layer\_norm\_ablation\_norn\_lr1e-3\_steps2000\_seed0}.
\end{itemize}

\textbf{What happens at the previous optimal LR?} Without RMSNorm, training becomes unstable and diverges quickly at \texttt{lr\_max=3e-3} (diverged at step \texttt{180}, before the first validation eval at step \texttt{200}).

\textbf{Can we regain stability with a lower LR?} Yes. Reducing to \texttt{lr\_max=1e-3} yields a stable run (final validation loss \texttt{1.6846} after \texttt{2000} steps), though it is still worse than the RMSNorm baseline.

\textbf{Learning curves:}
\begin{center}
      \includegraphics[width=0.98\linewidth]{figures/layer_norm_ablation_lr3e-3.pdf}

      \vspace{2pt}
      \small\emph{RMSNorm removed @ \texttt{lr\_max=3e-3} (diverges). Baseline is truncated to the first 2000 steps for comparison.}
\end{center}

\begin{center}
      \includegraphics[width=0.98\linewidth]{figures/layer_norm_ablation_lr1e-3.pdf}

      \vspace{2pt}
      \small\emph{RMSNorm removed @ \texttt{lr\_max=1e-3} (stable). Baseline is truncated to the first 2000 steps for comparison.}
\end{center}

\textbf{Commentary:} RMSNorm substantially improves optimization stability at higher learning rates. Removing it shrinks the stable learning-rate range (here, \texttt{3e-3} diverges without RMSNorm) and even after retuning to a lower LR (\texttt{1e-3}), performance degrades (higher validation loss) compared to the baseline.
\end{problem}

\begin{problem}{no\_pos\_emb}{Implement NoPE (1 point)}
Modify your Transformer implementation with RoPE to remove the position embedding information entirely, and see what happens.

Deliverable: A learning curve comparing the performance of RoPE and NoPE.

\textbf{answer: }I implemented a NoPE switch by disabling RoPE in attention (no positional information is injected).

\textbf{Runs:}
\begin{itemize}
      \item Baseline (RoPE): \ttbreak{outputs/runs/20260130T032122Z}.
      \item NoPE (RoPE disabled): \ttbreak{outputs/runs/no\_pos\_emb\_nope\_lr3e-3\_steps2000\_seed0}.
\end{itemize}

\textbf{Learning curve (RoPE vs NoPE):}
\begin{center}
      \IfFileExists{figures/no_pos_emb_rope_vs_nope.pdf}{%
            \includegraphics[width=0.98\linewidth]{figures/no_pos_emb_rope_vs_nope.pdf}
      }{%
            \fbox{\parbox{0.95\linewidth}{\centering Figure missing: \texttt{figures/no\_pos\_emb\_rope\_vs\_nope.pdf}\\
                        Run the NoPE experiment, export TSVs, then run the plotting script.}}
      }
\end{center}

\textbf{Commentary:} Removing positional information degrades performance. In my short sweep (2000 steps), the best NoPE run occurred at \texttt{lr\_max=3e-3} with final validation loss \texttt{1.6630}, while the RoPE baseline achieves \texttt{1.6266} validation loss at step \texttt{2000} (and \texttt{1.3713} when trained to \texttt{10000} steps). Qualitatively, NoPE learns but converges to a worse validation loss than RoPE, consistent with positional information being important for language modeling.
\end{problem}

\begin{problem}{swiglu\_ablation}{SwiGLU vs. SiLU (1 point)}
Deliverable: A learning curve comparing the performance of SwiGLU and SiLU feed-forward networks, with approximately matched parameter counts.
Deliverable: A few sentences discussing your findings.

\textbf{answer: }I added an FFN variant switch: the default uses SwiGLU, and the ablation replaces the gated SwiGLU with a non-gated 2-layer SiLU MLP. To approximately match parameter counts, the SiLU hidden size is set to about $1.5\times$ the SwiGLU hidden size (rounded up to a multiple of 64).

\textbf{Runs:}
\begin{itemize}
      \item SwiGLU FFN: \ttbreak{outputs/runs/swiglu\_ablation\_swiglu\_lr3e-3\_steps2000\_seed0}.
      \item SiLU FFN (matched params): \ttbreak{outputs/runs/swiglu\_ablation\_silu\_lr3e-3\_steps2000\_seed0}.
\end{itemize}

\textbf{Learning curve (SwiGLU vs SiLU):}
\begin{center}
      \IfFileExists{figures/swiglu_ablation_swiglu_vs_silu.pdf}{%
            \includegraphics[width=0.98\linewidth]{figures/swiglu_ablation_swiglu_vs_silu.pdf}
      }{%
            \fbox{\parbox{0.95\linewidth}{\centering Figure missing: \texttt{figures/swiglu\_ablation\_swiglu\_vs\_silu.pdf}\\
                        Run the ablation experiment, export TSVs, then run the plotting script.}}
      }
\end{center}

\textbf{Discussion:} With approximately matched parameter counts, SwiGLU and SiLU perform very similarly in this setup. At \texttt{lr\_max=3e-3} for \texttt{2000} steps, the SwiGLU run reaches final validation loss \texttt{1.5918}, while the SiLU run reaches \texttt{1.5939}. This suggests the gating in SwiGLU is not dramatically better here, but it is at least competitive (slightly better on this run).
\end{problem}

\subsection{Running on OpenWebText}
\begin{problem}{main\_experiment}{Experiment on OWT (2 points)}
Train your language model on OpenWebText with the same model architecture and total training iterations as TinyStories. How well does this model do? Deliverable: A learning curve of your language model on OpenWebText. Describe the difference in losses from TinyStories – how should we interpret these losses?
Deliverable: Generated text from OpenWebText LM, in the same format as the TinyStories outputs. How is the fluency of this text? Why is the output quality worse even though we have the same model and compute budget as TinyStories?

\textbf{answer: }I trained the same 17M-ish Transformer LM configuration as the TinyStories baseline (ctx=256, 4L/16H/512d/1344ff, RoPE+RMSNorm+SwiGLU, 10k steps) on the provided OpenWebText (OWT) sample.

\textbf{Tokenization: }I pretokenized OWT using my OWT-trained 32K BPE tokenizer (\texttt{outputs/owt\_vocab.json} + \texttt{outputs/owt\_merges.txt}). Accordingly, I trained the LM with \texttt{vocab\_size=32000} so token IDs stay in range.

\textbf{Run directory: }\ttbreak{outputs/runs/main\_experiment\_owt\_lr3e-3\_steps10000\_seed0-478175}

\textbf{Final metrics: }final validation loss \texttt{4.0748} after \texttt{10000} steps (\texttt{327,680,000} tokens processed), wallclock \texttt{1587.5s} (\texttt{26.5} minutes; \(\approx 2.06\times 10^5\) tokens/s).

\textbf{Learning curve (OWT):}
\begin{center}
      \IfFileExists{figures/main_experiment_owt_learning_curve.pdf}{%
            \includegraphics[width=0.98\linewidth]{figures/main_experiment_owt_learning_curve.pdf}
      }{%
            \fbox{\parbox{0.95\linewidth}{\centering Figure missing: \texttt{figures/main\_experiment\_owt\_learning\_curve.pdf}\\
                        After the OWT run finishes, create TSVs and plot:\\
                        \ttbreak{uv run python experiments/metrics\_split\_to\_tsv.py outputs/runs/main\_experiment\_owt\_lr3e-3\_steps10000\_seed0/metrics.jsonl}\\
                        \ttbreak{uv run python experiments/metrics\_split\_to\_tsv.py outputs/runs/main\_experiment\_owt\_lr3e-3\_steps10000\_seed0-478175/metrics.jsonl}\\
                        \ttbreak{uv run python experiments/plot\_main\_experiment\_owt\_figures.py --owt-run outputs/runs/main\_experiment\_owt\_lr3e-3\_steps10000\_seed0-478175 --ts-run outputs/runs/20260130T032122Z}}}
      }
\end{center}

\textbf{Generated text (OWT LM):}
\IfFileExists{../outputs/runs/main_experiment_owt_lr3e-3_steps10000_seed0-478175/generated_text_clean.txt}{%
      \inputminted[fontsize=\small,breaklines]{text}{../outputs/runs/main_experiment_owt_lr3e-3_steps10000_seed0-478175/generated_text_clean.txt}
}{%
      \fbox{\parbox{0.95\linewidth}{\centering Missing text: \texttt{generated\_text\_clean.txt}\\
                  Generate from the final checkpoint (on \texttt{cs336dev}):\\
                  \ttbreak{uv run python experiments/generate\_text.py --run-dir outputs/runs/main\_experiment\_owt\_lr3e-3\_steps10000\_seed0-478175 --vocab outputs/owt\_vocab.json --merges outputs/owt\_merges.txt --prompt "The following is a story about " --max-new-tokens 512 --temperature 0.9 --top-p 0.95 --ban-eos}\\
                  Then copy \texttt{generated\_text.txt} to \texttt{generated\_text\_clean.txt} if desired.}}
}

\textbf{Interpreting losses:} The OWT validation loss (\texttt{4.0748}) is much higher than the TinyStories baseline (\texttt{1.3713} at 10k steps; run \ttbreak{outputs/runs/20260130T032122Z}). This is expected because OWT is far more diverse and noisy, with longer dependencies and many more plausible next tokens per context, so the intrinsic per-token entropy is higher. Therefore, loss values are strongly dataset-dependent and should not be compared across datasets as an absolute measure of "model quality".

\textbf{Fluency and why it can look worse on OWT:} In my sample, the model produces news-like fragments with some topical coherence (international politics), but it quickly becomes rambling and repetitive (e.g., repeated country names) and loses entity-level consistency. With the same compute budget, the model sees far fewer repeats of simple patterns than TinyStories, and OWT has wider style/genre diversity and longer dependencies, which a small model struggles to model; decoding choices (temperature/top-p, EOS handling) can further reduce apparent coherence.
\end{problem}

\end{document}
